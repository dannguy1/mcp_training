# Deployable Model Package Specification

This document provides a comprehensive specification for the deployable model packages generated by the MCP Training Service. It describes the package structure, contents, validation procedures, and integration patterns for consuming systems.

---

## Package Overview

### Package Format
- **Container**: ZIP archive
- **Naming Convention**: `model_{version}_deployment.zip`
- **Version Format**: ISO timestamp (e.g., `20241201_143022`)
- **Package Version**: 1.0 (specified in deployment manifest)

### Package Creation
Packages are automatically generated when a model is deployed through the MCP Training Service web interface. The deployment process:
1. Marks the model as "deployed" in the registry
2. Creates a comprehensive deployment package
3. Includes all necessary files for production use
4. Generates integrity checks and validation scripts

---

## Package Structure

```
model_{version}_deployment.zip
â”œâ”€â”€ model.joblib                    # Trained model file
â”œâ”€â”€ scaler.joblib                   # Feature scaler (if applicable)
â”œâ”€â”€ metadata.json                   # Complete model metadata
â”œâ”€â”€ deployment_manifest.json        # Deployment configuration & integrity
â”œâ”€â”€ validate_model.py              # Model validation script
â”œâ”€â”€ inference_example.py           # Production-ready inference class
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # Comprehensive documentation
```

---

## File Specifications

### 1. Core Model Files

#### `model.joblib`
- **Purpose**: Serialized trained model
- **Format**: Joblib serialization
- **Content**: Trained scikit-learn model (typically IsolationForest)
- **Size**: Variable (typically 50KB - 2MB)
- **Usage**: Loaded by inference scripts

#### `scaler.joblib`
- **Purpose**: Feature normalization scaler
- **Format**: Joblib serialization
- **Content**: StandardScaler or similar preprocessing
- **Optional**: May not exist if no scaling was applied
- **Usage**: Applied to input features before prediction

#### `metadata.json`
- **Purpose**: Complete model information
- **Format**: JSON
- **Content**: Training details, evaluation metrics, model parameters
- **Structure**: See Metadata Schema section below

### 2. Deployment Configuration

#### `deployment_manifest.json`
- **Purpose**: Deployment configuration and integrity verification
- **Format**: JSON
- **Content**: Package metadata, file hashes, inference configuration
- **Structure**: See Deployment Manifest Schema section below

### 3. Validation & Integration

#### `validate_model.py`
- **Purpose**: Model integrity and functionality validation
- **Format**: Python script
- **Dependencies**: joblib, numpy, scikit-learn
- **Functions**:
  - File integrity verification (SHA256 hashes)
  - Model loading validation
  - Basic inference testing
  - Feature compatibility checking

#### `inference_example.py`
- **Purpose**: Production-ready inference implementation
- **Format**: Python module
- **Class**: `ModelInference`
- **Features**:
  - Automatic model loading
  - Feature preprocessing
  - Batch prediction support
  - Error handling
  - Configuration management

#### `requirements.txt`
- **Purpose**: Python dependency specification
- **Format**: pip requirements file
- **Dependencies**:
  - joblib >= 1.3.0
  - numpy >= 1.21.0
  - scikit-learn >= 1.0.0
  - pandas >= 1.3.0

#### `README.md`
- **Purpose**: Comprehensive usage documentation
- **Format**: Markdown
- **Content**:
  - Model information and metrics
  - Quick start guide
  - Integration examples
  - Troubleshooting guide
  - Performance monitoring tips

---

## Schema Specifications

### Metadata Schema (`metadata.json`)

```json
{
  "model_info": {
    "version": "string",
    "created_at": "ISO8601 timestamp",
    "model_type": "string",
    "training_source": "string",
    "export_file": "string",
    "training_id": "string"
  },
  "training_info": {
    "training_samples": "integer",
    "feature_names": ["string"],
    "feature_count": "integer",
    "export_file_size": "integer",
    "training_duration": "float",
    "model_parameters": "object"
  },
  "evaluation_info": {
    "basic_metrics": {
      "accuracy": "float",
      "precision": "float",
      "recall": "float",
      "f1_score": "float",
      "threshold_value": "float",
      "anomaly_ratio": "float"
    },
    "cross_validation_score": "float",
    "feature_importance": "object"
  },
  "deployment_info": {
    "status": "string",
    "deployed_at": "ISO8601 timestamp",
    "deployed_by": "string"
  }
}
```

### Deployment Manifest Schema (`deployment_manifest.json`)

```json
{
  "model_version": "string",
  "deployment_timestamp": "ISO8601 timestamp",
  "package_format_version": "string",
  "model_info": {
    "model_type": "string",
    "training_source": "string",
    "training_id": "string",
    "export_file": "string"
  },
  "training_info": {
    "training_samples": "integer",
    "feature_names": ["string"],
    "feature_count": "integer",
    "export_file_size": "integer",
    "training_duration": "float",
    "model_parameters": "object"
  },
  "evaluation_info": {
    "basic_metrics": "object",
    "cross_validation_score": "float",
    "feature_importance": "object"
  },
  "deployment_info": {
    "status": "string",
    "deployed_at": "string",
    "deployed_by": "string"
  },
  "files": {
    "model_file": "string",
    "scaler_file": "string",
    "metadata_file": "string",
    "validation_script": "string",
    "inference_example": "string",
    "requirements": "string"
  },
  "file_integrity": {
    "model.joblib": "SHA256 hash",
    "scaler.joblib": "SHA256 hash",
    "metadata.json": "SHA256 hash"
  },
  "inference_config": {
    "threshold": "float",
    "anomaly_ratio": "float",
    "score_percentile": "integer",
    "batch_size": "integer",
    "timeout_seconds": "integer"
  },
  "model_artifacts": {
    "model_size_bytes": "integer",
    "scaler_size_bytes": "integer",
    "total_package_size": "integer"
  }
}
```

---

## Package Validation

### Integrity Verification

The package includes comprehensive integrity checks:

1. **File Hash Verification**: SHA256 hashes for all core files
2. **Model Loading Test**: Ensures model can be deserialized
3. **Feature Compatibility**: Validates feature names and count
4. **Inference Testing**: Basic prediction functionality test

### Validation Process

```bash
# Extract package
unzip model_{version}_deployment.zip
cd model_{version}_deployment

# Install dependencies
pip install -r requirements.txt

# Run validation
python validate_model.py
```

**Expected Output**:
```
ðŸ” Validating model deployment...
ðŸ“ Checking file integrity...
âœ… model.joblib: OK
âœ… scaler.joblib: OK
âœ… metadata.json: OK
ðŸ¤– Loading model...
âœ… Model loaded successfully
âœ… Scaler loaded successfully
ðŸ§ª Testing model inference...
âœ… Inference test successful
   Sample predictions: [0, 0, 1, 0, 0]
   Score range: 0.123 to 0.987
ðŸŽ‰ Model validation completed successfully!
```

---

## Integration Patterns

### Pattern 1: Direct Integration

```python
from inference_example import ModelInference

# Initialize model
inference = ModelInference()

# Prepare features (must match training feature names)
features = [
    {
        'auth_failure_ratio': 0.1,
        'deauth_ratio': 0.05,
        'beacon_ratio': 0.3,
        'unique_mac_count': 15,
        'unique_ssid_count': 8,
        'mean_signal_strength': -45.0,
        'std_signal_strength': 5.0,
        'mean_data_rate': 54.0,
        'mean_packet_loss': 0.02,
        'error_ratio': 0.01,
        'warning_ratio': 0.03,
        'mean_hour_of_day': 14.0,
        'mean_day_of_week': 3.0,
        'mean_time_between_events': 120.0,
        'total_devices': 25,
        'max_device_activity': 0.8,
        'mean_device_activity': 0.4
    }
]

# Make predictions
result = inference.predict(features)
print(f"Anomalies: {result['anomaly_count']}/{result['total_samples']}")
```

### Pattern 2: API Service

```python
from flask import Flask, request, jsonify
from inference_example import ModelInference

app = Flask(__name__)
inference = ModelInference()

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        features = data.get('features', [])
        
        if not features:
            return jsonify({'error': 'No features provided'}), 400
        
        result = inference.predict(features)
        return jsonify(result)
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### Pattern 3: Batch Processing

```python
from inference_example import ModelInference
import pandas as pd

# Initialize model
inference = ModelInference()

# Load data in batches
def process_batches(data_file, batch_size=1000):
    for chunk in pd.read_csv(data_file, chunksize=batch_size):
        # Convert to required format
        features = chunk.to_dict('records')
        
        # Make predictions
        result = inference.predict(features)
        
        # Process results
        yield result

# Process large dataset
for batch_result in process_batches('large_dataset.csv'):
    print(f"Batch anomalies: {batch_result['anomaly_count']}")
```

---

## ModelInference Class API

### Constructor
```python
ModelInference(model_dir: str = ".")
```
- **model_dir**: Path to extracted package directory (default: current directory)

### Methods

#### `preprocess_features(features: List[Dict[str, Any]]) -> np.ndarray`
- Converts feature dictionaries to numpy array
- Ensures feature order matches training data
- Handles missing features (defaults to 0.0)

#### `predict(features: List[Dict[str, Any]]) -> Dict[str, Any]`
- Main prediction method
- Returns dictionary with:
  - `predictions`: List of binary predictions (0=normal, 1=anomaly)
  - `scores`: List of anomaly scores
  - `threshold`: Detection threshold used
  - `anomaly_count`: Number of anomalies detected
  - `total_samples`: Total number of samples processed

### Properties
- `manifest`: Deployment manifest data
- `model`: Loaded scikit-learn model
- `scaler`: Feature scaler (if available)
- `threshold`: Detection threshold
- `feature_names`: List of expected feature names

---

## Configuration Management

### Inference Configuration

The deployment manifest includes inference configuration:

```json
{
  "inference_config": {
    "threshold": 0.5,
    "anomaly_ratio": 0.1,
    "score_percentile": 90,
    "batch_size": 1000,
    "timeout_seconds": 30
  }
}
```

### Environment Variables

For production deployment, consider these environment variables:

```bash
# Model configuration
MODEL_THRESHOLD=0.5
MODEL_BATCH_SIZE=1000
MODEL_TIMEOUT=30

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Performance
WORKER_PROCESSES=4
MAX_MEMORY_MB=2048
```

---

## Error Handling

### Common Error Scenarios

1. **Feature Mismatch**
   ```python
   # Error: Missing required features
   ValueError: Feature 'auth_failure_ratio' not found in input
   
   # Solution: Ensure all training features are provided
   ```

2. **Model Loading Failure**
   ```python
   # Error: Corrupted model file
   joblib.UnpicklingError: Invalid pickle data
   
   # Solution: Re-download package and verify integrity
   ```

3. **Memory Issues**
   ```python
   # Error: Large batch processing
   MemoryError: Unable to allocate array
   
   # Solution: Reduce batch size or process in chunks
   ```

### Error Recovery

```python
from inference_example import ModelInference
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def safe_predict(features, max_retries=3):
    for attempt in range(max_retries):
        try:
            inference = ModelInference()
            return inference.predict(features)
        except Exception as e:
            logger.error(f"Prediction attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(1)  # Brief delay before retry
```

---

## Performance Considerations

### Memory Usage
- **Model Loading**: ~50-200MB RAM
- **Feature Processing**: ~10-50MB per 1000 samples
- **Batch Processing**: Scale with batch size

### Processing Speed
- **Single Prediction**: ~1-5ms
- **Batch Processing**: ~100-500 predictions/second
- **Scalability**: Linear with CPU cores

### Optimization Tips
1. **Batch Processing**: Use appropriate batch sizes (500-2000)
2. **Memory Management**: Process large datasets in chunks
3. **Caching**: Reuse ModelInference instance for multiple predictions
4. **Parallel Processing**: Use multiple workers for high-throughput scenarios

---

## Security Considerations

### File Integrity
- All core files include SHA256 hashes
- Validation script verifies file integrity
- Package tampering detection

### Input Validation
- Feature type and range validation
- Malicious input protection
- Resource usage limits

### Access Control
- Model file permissions
- API authentication (if applicable)
- Audit logging

---

## Monitoring & Maintenance

### Performance Metrics
- Prediction latency
- Throughput (predictions/second)
- Memory usage
- Error rates

### Model Drift Detection
- Feature distribution monitoring
- Prediction confidence tracking
- Performance degradation alerts

### Update Procedures
1. **Validation**: Test new model with validation script
2. **A/B Testing**: Compare old vs new model performance
3. **Rollback Plan**: Maintain previous model version
4. **Monitoring**: Enhanced monitoring during transition

---

## Troubleshooting Guide

### Package Extraction Issues
```bash
# Verify package integrity
unzip -t model_{version}_deployment.zip

# Check file permissions
ls -la model_{version}_deployment/

# Validate file hashes
python validate_model.py
```

### Import Errors
```bash
# Check Python version compatibility
python --version

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "import joblib, numpy, sklearn; print('OK')"
```

### Performance Issues
```python
# Profile memory usage
import psutil
process = psutil.Process()
print(f"Memory usage: {process.memory_info().rss / 1024 / 1024:.1f} MB")

# Optimize batch size
for batch_size in [100, 500, 1000, 2000]:
    # Test performance with different batch sizes
    pass
```

---

## Support & Documentation

### Package Contents
- **README.md**: Comprehensive usage guide
- **inference_example.py**: Working code examples
- **validate_model.py**: Self-diagnostic tool

### Additional Resources
- Model training logs and metrics
- Feature engineering documentation
- Performance benchmarks
- Integration examples

### Contact Information
For issues with model deployment packages:
- Check validation script output
- Review deployment manifest
- Contact ML team with model version and error details

---

## Version History

### Package Format Version 1.0
- Initial release
- Comprehensive validation and documentation
- Production-ready inference class
- Industry-standard security practices

### Future Enhancements
- Model versioning and rollback support
- Advanced monitoring integration
- Automated performance optimization
- Cloud deployment templates

---

This specification ensures that deployable packages are self-contained, well-documented, and ready for production use. The comprehensive validation and integration patterns enable seamless deployment across different environments and use cases. 